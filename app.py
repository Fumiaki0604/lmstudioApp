import time
from datetime import datetime

import requests
import trafilatura
import streamlit as st

# -----------------------------
# Constants / Utilities
# -----------------------------
DEFAULT_UA = (
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
    "AppleWebKit/537.36 (KHTML, like Gecko) "
    "Chrome/123.0 Safari/537.36"
)

def lmstudio_models(base_url: str, timeout: int = 3):
    """LM StudioãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã€ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¿”ã™ã€‚"""
    r = requests.get(base_url.rstrip("/") + "/models", timeout=timeout)
    r.raise_for_status()
    data = r.json()
    return [m.get("id") for m in data.get("data", []) if m.get("id")]

def fetch_html(url: str, timeout: int = 20) -> str:
    r = requests.get(url, timeout=timeout, headers={"User-Agent": DEFAULT_UA})
    r.raise_for_status()
    return r.text

def extract_main_text(html: str) -> str:
    text = trafilatura.extract(
        html,
        output_format="txt",     # â† trafilaturaã®ä»•æ§˜ã«åˆã‚ã›ã¦ txt
        include_comments=False,
        include_tables=True,
        favor_precision=True,
    )
    return (text or "").strip()

def build_prompt(url: str, text: str, max_chars: int) -> str:
    # é•·æ–‡å¯¾ç­–ï¼šå†’é ­70% + æœ«å°¾30%ï¼ˆæƒ…å ±ã®åã‚Šã‚’å°‘ã—æ¸›ã‚‰ã™ï¼‰
    if len(text) <= max_chars:
        clipped = text
    else:
        head = text[: int(max_chars * 0.7)]
        tail = text[-int(max_chars * 0.3):]
        clipped = head + "\n\n...(ä¸­ç•¥)...\n\n" + tail

    return f"""æ¬¡ã®Webãƒšãƒ¼ã‚¸æœ¬æ–‡ã‚’è¦ç´„ã—ã¦ãã ã•ã„ã€‚

åˆ¶ç´„:
- é‡è¦ãƒã‚¤ãƒ³ãƒˆã‚’ç®‡æ¡æ›¸ãï¼ˆ5ã€œ10å€‹ï¼‰
- æ•°å€¤ãƒ»å›ºæœ‰åè©ãƒ»çµè«–ã¯è½ã¨ã•ãªã„
- å¯èƒ½ãªã‚‰ã€Œæ„æ€æ±ºå®šã®æ³¨æ„ç‚¹ã€ã‚‚1ã€œ3å€‹

URL: {url}

æœ¬æ–‡:
\"\"\"\n{clipped}\n\"\"\"
"""

def call_lmstudio_chat(
    base_url: str,
    model: str,
    prompt: str,
    temperature: float = 0.2,
    max_tokens: int = 800,
    timeout: int = 180,
) -> str:
    endpoint = base_url.rstrip("/") + "/chat/completions"
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": "ã‚ãªãŸã¯ãƒ—ãƒ­ã®è¦ç´„è€…ã§ã™ã€‚æ—¥æœ¬èªã§ç°¡æ½”ã«è¦ç´„ã—ã€é‡è¦ãƒã‚¤ãƒ³ãƒˆã¨æ³¨æ„ç‚¹ã‚’æ•´ç†ã—ã¦ãã ã•ã„ã€‚"},
            {"role": "user", "content": prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
        "stream": False,
    }
    r = requests.post(endpoint, json=payload, timeout=timeout)
    r.raise_for_status()
    data = r.json()
    return data["choices"][0]["message"]["content"]

# -----------------------------
# Dynamic UI helper text
# -----------------------------
def label_max_chars(n: int) -> str:
    if n <= 2500:
        return "âš¡ ã‹ãªã‚Šé«˜é€Ÿï¼ˆè¦ç‚¹ä¸­å¿ƒï¼‰ã€‚é•·ã„è¨˜äº‹ã ã¨æŠœã‘ãŒå‡ºã‚„ã™ã„ã€‚"
    if n <= 4000:
        return "ğŸš´ é«˜é€Ÿå¯„ã‚Šï¼ˆã ã„ãŸã„å¤–ã•ãªã„ï¼‰ã€‚æ™®æ®µä½¿ã„ã«ã¡ã‚‡ã†ã©è‰¯ã„ã€‚"
    if n <= 6000:
        return "âš–ï¸ ãƒãƒ©ãƒ³ã‚¹ï¼ˆæ–‡è„ˆã®å–ã‚Šã“ã¼ã—ãŒæ¸›ã‚‹ï¼‰ã€‚å°‘ã—é‡ããªã‚‹ã€‚"
    if n <= 9000:
        return "ğŸ§  é«˜ç²¾åº¦ï¼ˆèƒŒæ™¯ã¾ã§æ‹¾ã„ã‚„ã™ã„ï¼‰ã€‚â³é…ããªã‚Šã‚„ã™ã„ï¼†ä¸Šé™æ³¨æ„ã€‚"
    return "ğŸ¢ ç‰¹ç››ï¼ˆè©³ç´°ã¾ã§ç²˜ã‚‹ï¼‰ã€‚â³æ™‚é–“ã‹ã‹ã‚‹ï¼‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¶…ãˆãƒªã‚¹ã‚¯é«˜ã‚ã€‚"

def label_max_tokens(t: int) -> str:
    if t <= 400:
        return "ğŸ§¾ è¶…çŸ­æ–‡ï¼ˆçµè«–ã ã‘ï¼‰ã€‚âš¡æœ€é€Ÿã€‚"
    if t <= 800:
        return "ğŸ“ æ¨™æº–ï¼ˆèª­ã¿ã‚„ã™ãè¦ç‚¹ãŒæƒã†ï¼‰ã€‚é€Ÿåº¦ã‚‚å®‰å®šã€‚"
    if t <= 1200:
        return "ğŸ“Œ ä¸å¯§ï¼ˆè£œè¶³ã‚‚å…¥ã‚‹ï¼‰ã€‚â³å°‘ã—é…ããªã‚‹ã€‚"
    return "ğŸ“š è©³ç´°ï¼ˆæŠœã‘ã‚’æ¸›ã‚‰ã™ï¼‰ã€‚â³é…ããªã‚Šã‚„ã™ã„ã€‚"

def label_temperature(x: float) -> str:
    if x <= 0.2:
        return "ğŸ§Š ã‹ãªã‚Šå …ã‚ï¼ˆãƒ–ãƒ¬ã«ãã„ï¼äº‹å®Ÿå¯„ã‚Šï¼‰ã€‚"
    if x <= 0.6:
        return "ğŸ™‚ ã¡ã‚‡ã†ã©è‰¯ã„ï¼ˆè‡ªç„¶ï¼å®‰å®šï¼‰ã€‚"
    if x <= 1.0:
        return "ğŸ¨ è¡¨ç¾è±Šã‹ï¼ˆè¨€ã„å›ã—ãŒå¢—ãˆã‚‹ï¼å°‘ã—ãƒ–ãƒ¬ã‚„ã™ã„ï¼‰ã€‚"
    return "ğŸ² æºã‚‰ãå¤§ï¼ˆç™ºæƒ³ã¯å‡ºã‚‹ãŒèª¤å·®ã‚‚å¢—ãˆãŒã¡ï¼‰ã€‚"

def speed_meter(max_chars: int, max_tokens: int) -> tuple[int, str]:
    """
    â€œä½“æ„Ÿç”¨â€ã®è¶…ã–ã£ãã‚Šé€Ÿåº¦ãƒ¡ãƒ¼ã‚¿ãƒ¼ã€‚
    å³å¯†ã§ã¯ãªãã€Œå¾…ã¡æ™‚é–“ã®ä¸å®‰ã€ã‚’æ¸›ã‚‰ã™ç›®çš„ã€‚
    """
    score = 100
    score -= int((max_chars - 1000) / 200)   # æ–‡å­—æ•°ãŒå¢—ãˆã‚‹ã»ã©é‡ã„
    score -= int((max_tokens - 100) / 30)    # å‡ºåŠ›ãŒå¢—ãˆã‚‹ã»ã©é‡ã„
    score = max(5, min(100, score))
    label = "é€Ÿã„" if score >= 70 else "ãµã¤ã†" if score >= 40 else "é…ã„"
    return score, label

# -----------------------------
# Streamlit UI
# -----------------------------
st.set_page_config(page_title="URLè¦ç´„ï¼ˆLM Studio / ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼‰", layout="centered")
st.title("URLè¦ç´„ï¼ˆLM Studio / ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼‰")

# session state for URL input
if "url" not in st.session_state:
    st.session_state["url"] = ""

with st.sidebar:
    st.header("æ¥ç¶šè¨­å®š")
    base_url = st.text_input("LM Studio Base URL", value="http://localhost:1234/v1")

    if st.button("ğŸ”„ æ¥ç¶šã‚’å†ç¢ºèª"):
        st.rerun()

    # --- æ¥ç¶šãƒã‚§ãƒƒã‚¯ï¼ˆå¸¸æ™‚è¡¨ç¤ºï¼‰ ---
    models = []
    lm_ok = False
    err = None
    t0 = time.time()
    try:
        models = lmstudio_models(base_url, timeout=3)
        lm_ok = True
    except Exception as e:
        err = e
    elapsed_ms = int((time.time() - t0) * 1000)
    checked_at = datetime.now().strftime("%H:%M:%S")

    if lm_ok:
        st.success(f"ğŸŸ¢ æ¥ç¶šä¸­ï¼ˆ{checked_at} / {elapsed_ms}msï¼‰")
        st.caption(f"æ¤œå‡ºãƒ¢ãƒ‡ãƒ«æ•°: {len(models)}")
        if models:
            st.caption(f"ä¾‹: {models[0]}")
    else:
        st.error(f"ğŸ”´ æœªæ¥ç¶šï¼ˆ{checked_at} / {elapsed_ms}msï¼‰")
        st.caption("LM Studioã§ãƒ¢ãƒ‡ãƒ«ã‚’Loadã—ã€Local Serverã‚’Runningã«ã—ã¦ãã ã•ã„ã€‚")
        st.caption(f"è©³ç´°: {err}")

    st.divider()

    st.header("ç”Ÿæˆè¨­å®š")

    max_chars = st.slider("å…¥åŠ›ã®æœ€å¤§æ–‡å­—æ•°", min_value=1000, max_value=12000, value=3500, step=500)
    st.caption(f"å…¥åŠ›: {label_max_chars(max_chars)}")

    max_tokens = st.slider("å‡ºåŠ›ãƒˆãƒ¼ã‚¯ãƒ³ä¸Šé™", min_value=100, max_value=2000, value=800, step=50)
    st.caption(f"å‡ºåŠ›: {label_max_tokens(max_tokens)}")

    temperature = st.slider("Temperature", min_value=0.0, max_value=1.5, value=0.2, step=0.1)
    st.caption(f"æ¸©åº¦: {label_temperature(temperature)}")

    score, speed_label = speed_meter(max_chars, max_tokens)
    st.progress(score)
    st.caption(f"ä½“æ„Ÿé€Ÿåº¦ã®ç›®å®‰: **{speed_label}**ï¼ˆã–ã£ãã‚Šï¼‰")

# æœªæ¥ç¶šãªã‚‰ãƒ¡ã‚¤ãƒ³ç”»é¢ã‚‚æ­¢ã‚ã‚‹ï¼ˆUXçš„ã«è¿·ã‚ã›ãªã„ï¼‰
if not lm_ok:
    st.stop()

# ãƒ¢ãƒ‡ãƒ«é¸æŠï¼ˆæ¥ç¶šã§ãã¦ã„ã‚‹å‰æï¼‰
default_model = "openai/gpt-oss-20b" if "openai/gpt-oss-20b" in models else (models[0] if models else "")
model = st.selectbox("ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«", options=models, index=models.index(default_model) if default_model in models else 0)

url = st.text_input("è¦ç´„ã—ãŸã„URLã‚’å…¥åŠ›", key="url", placeholder="https://...")

col1, col2 = st.columns([1, 1])
with col1:
    run = st.button("è¦ç´„ã™ã‚‹", type="primary")
with col2:
    if st.button("ã‚¯ãƒªã‚¢"):
        st.session_state["url"] = ""
        st.rerun()

if run:
    if not url.strip():
        st.warning("URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # é€²æ—ã‚’æ®µéšè¡¨ç¤ºï¼ˆä½“æ„Ÿé€Ÿåº¦UPï¼‰
    with st.status("å‡¦ç†ä¸­...", expanded=True) as status:
        try:
            status.update(label="ğŸŒ ãƒšãƒ¼ã‚¸å–å¾—ä¸­...", state="running")
            html = fetch_html(url.strip(), timeout=20)

            status.update(label="âœ‚ï¸ æœ¬æ–‡æŠ½å‡ºä¸­...", state="running")
            text = extract_main_text(html)
            if not text:
                status.update(label="æœ¬æ–‡æŠ½å‡ºã«å¤±æ•—", state="error")
                st.error("æœ¬æ–‡æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆJSæç”»/ãƒ–ãƒ­ãƒƒã‚¯/æœ¬æ–‡ãªã—ã®å¯èƒ½æ€§ï¼‰ã€‚Playwrightç‰ˆãŒå¿…è¦ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚")
                st.stop()

            status.update(label="ğŸ§  è¦ç´„ç”Ÿæˆä¸­...", state="running")
            prompt = build_prompt(url.strip(), text, max_chars=max_chars)
            summary = call_lmstudio_chat(
                base_url=base_url,
                model=model,
                prompt=prompt,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=180,
            )

            status.update(label="âœ… å®Œäº†", state="complete", expanded=False)

        except Exception as e:
            status.update(label="âŒ ã‚¨ãƒ©ãƒ¼", state="error", expanded=True)
            st.exception(e)
            st.stop()

    st.subheader("è¦ç´„çµæœ")
    st.markdown(summary)

    with st.expander("æŠ½å‡ºã—ãŸæœ¬æ–‡ï¼ˆå…ˆé ­ï¼‰ã‚’è¦‹ã‚‹"):
        st.text(text[:2000])